{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RanxduG/Translation_Transformer/blob/main/Colab_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEA_Mw3TzH-f"
      },
      "source": [
        "##Install required packages testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4d-cbkmol7P",
        "outputId": "0ad8ca43-4df0-4469-fd1f-62e8dfa955dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers datasets\n",
        "!pip install matplotlib seaborn\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMRpmFrizL0R"
      },
      "source": [
        "##Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jL_OEKwIxrWl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIL0GMfhzP1E"
      },
      "source": [
        "##Initializing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "5de21bea4bb24538b5a07c905df753f2",
            "523671f1674a45a3bbee1fb69003a174",
            "de2d632ab64f4db1a1334823c8f6f2da",
            "8812cf8b23614cc190f0279566f6a180",
            "0b67d37cc6924b22b0524353c8773e3c",
            "4d59c3538c7e4954989d67046a0b0fc2",
            "3824fd9551104011b103d0772fb1b6ce",
            "03370dcb5262415bbe8e312f933c6805",
            "3799132bc11c4488bd2e73a7ab1fdd47",
            "affc57d1185a43c68330cd6a6857f102",
            "3ec26f5619fe4b76a83256ad9c2f3c8d"
          ]
        },
        "id": "wWdFSYG6xrTu",
        "outputId": "544a96f4-ec5f-4602-bbaa-4c2b10cccda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading FLORES-200 dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading FLORES-200: Loading a dataset cached in a LocalFileSystem is not supported.\n",
            "Falling back to alternative dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5de21bea4bb24538b5a07c905df753f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading alternative dataset: Invalid pattern: '**' can only be an entire path component\n",
            "Using sample dataset with real Sinhala script...\n",
            "\n",
            "Sample data:\n",
            "English: Hello\n",
            "Sinhala: ආයුබෝවන්\n",
            "---\n",
            "English: How are you?\n",
            "Sinhala: ඔබ කෙසේද?\n",
            "---\n",
            "English: Good morning\n",
            "Sinhala: සුභ උදෑසනක්\n",
            "---\n",
            "English: Good evening\n",
            "Sinhala: සුභ සන්ධ්‍යාවක්\n",
            "---\n",
            "English: Thank you\n",
            "Sinhala: ස්තූතියි\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Upload your transformer_sinhala.py file to Colab and import it\n",
        "from transformer import Transformer, get_device\n",
        "\n",
        "# Set device\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load dataset from HuggingFace\n",
        "print(\"Loading FLORES-200 dataset...\")\n",
        "try:\n",
        "    # Load FLORES-200 dataset\n",
        "    dataset = load_dataset(\"facebook/flores\", \"eng_Latn-sin_Sinh\")\n",
        "\n",
        "    # Extract English and Sinhala sentences\n",
        "    english_sentences = []\n",
        "    sinhala_sentences = []\n",
        "\n",
        "    # Use dev split for smaller dataset (good for Colab)\n",
        "    data_split = dataset['dev']\n",
        "\n",
        "    for example in data_split:\n",
        "        english_sentences.append(example['sentence_eng_Latn'])\n",
        "        sinhala_sentences.append(example['sentence_sin_Sinh'])\n",
        "\n",
        "    print(f\"Loaded {len(english_sentences)} sentence pairs from FLORES-200\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FLORES-200: {e}\")\n",
        "    print(\"Falling back to alternative dataset...\")\n",
        "\n",
        "    # Alternative: Load sentence alignment dataset\n",
        "    try:\n",
        "        dataset = load_dataset(\"NLPC-UOM/sentence_alignment_dataset-Sinhala-Tamil-English\")\n",
        "\n",
        "        # Extract English and Sinhala sentences\n",
        "        english_sentences = []\n",
        "        sinhala_sentences = []\n",
        "\n",
        "        # Use train split and limit to first 1000 examples\n",
        "        data_split = dataset['train']\n",
        "\n",
        "        for i, example in enumerate(data_split):\n",
        "            if i >= 1000:  # Limit dataset size for Colab\n",
        "                break\n",
        "            if 'english' in example and 'sinhala' in example:\n",
        "                if example['english'] and example['sinhala']:\n",
        "                    english_sentences.append(example['english'].strip())\n",
        "                    sinhala_sentences.append(example['sinhala'].strip())\n",
        "\n",
        "        print(f\"Loaded {len(english_sentences)} sentence pairs from NLPC-UOM dataset\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"Error loading alternative dataset: {e2}\")\n",
        "        print(\"Using sample dataset with real Sinhala script...\")\n",
        "\n",
        "        # Fallback to sample dataset with real Sinhala script\n",
        "        # Sample dataset with cleaned data\n",
        "        english_sentences = [\n",
        "            \"Hello\",\n",
        "            \"How are you?\",\n",
        "            \"Good morning\",\n",
        "            \"Good evening\",\n",
        "            \"Thank you\",\n",
        "            \"You are welcome\",\n",
        "            \"I am fine\",\n",
        "            \"What is your name?\",\n",
        "            \"Nice to meet you\",\n",
        "            \"How old are you?\",\n",
        "            \"Where are you from?\",\n",
        "            \"I love you\",\n",
        "            \"Goodbye\",\n",
        "            \"See you later\",\n",
        "            \"Have a nice day\",\n",
        "            \"I am hungry\",\n",
        "            \"I am tired\",\n",
        "            \"What time is it?\",\n",
        "            \"It is raining\",\n",
        "            \"The weather is nice\",\n",
        "            \"I want to eat\",\n",
        "            \"I want to sleep\",\n",
        "            \"I am happy\",\n",
        "            \"I am sad\",\n",
        "            \"Help me\",\n",
        "            \"I do not understand\",\n",
        "            \"Speak slowly\",\n",
        "            \"Can you help me?\",\n",
        "            \"Where is the bathroom?\",\n",
        "            \"How much does it cost?\",\n",
        "            \"I am learning Sinhala\",\n",
        "            \"The book is on the table\",\n",
        "            \"I like this food\",\n",
        "            \"The car is red\",\n",
        "            \"She is beautiful\",\n",
        "            \"He is tall\",\n",
        "            \"We are friends\",\n",
        "            \"They are students\",\n",
        "            \"I go to school\",\n",
        "            \"I work in an office\"\n",
        "        ]\n",
        "\n",
        "        sinhala_sentences = [\n",
        "            \"ආයුබෝවන්\",\n",
        "            \"ඔබ කෙසේද?\",\n",
        "            \"සුභ උදෑසනක්\",\n",
        "            \"සුභ සන්ධ්‍යාවක්\",\n",
        "            \"ස්තූතියි\",\n",
        "            \"ආයුබෝවන්\",\n",
        "            \"මම හොඳයි\",\n",
        "            \"ඔබගේ නම මොකක්ද?\",\n",
        "            \"ඔබව හමුවීමට සතුටුයි\",\n",
        "            \"ඔබට වයස කීයද?\",\n",
        "            \"ඔබ කොහෙන්ද?\",\n",
        "            \"මම ඔබට ආදරෙයි\",\n",
        "            \"ආයුබෝවන්\",\n",
        "            \"ඉදිරියෙදී හමුවෙමු\",\n",
        "            \"හොඳ දවසක් ගත කරන්න\",\n",
        "            \"මට කිඩෙන් හිතෙනවා\",\n",
        "            \"මට කඩා වැටෙනවා\",\n",
        "            \"වේලාව කීයද?\",\n",
        "            \"වැස්ස එනවා\",\n",
        "            \"කාලගුණය හොඳයි\",\n",
        "            \"මට කන්න ඕන\",\n",
        "            \"මට නිදාගන්න ඕන\",\n",
        "            \"මට සතුටුයි\",\n",
        "            \"මට දුකයි\",\n",
        "            \"මට උදව් කරන්න\",\n",
        "            \"මට තේරෙන්නේ නෑ\",\n",
        "            \"සෙමින් කතා කරන්න\",\n",
        "            \"ඔබට මට උදව් කරන්න පුළුවන්ද?\",\n",
        "            \"නාන කාමරය කොහෙද?\",\n",
        "            \"මේක කීයද?\",\n",
        "            \"මම සිංහල ඉගෙන ගන්නවා\",\n",
        "            \"පොත මේසය උඩින් තියෙනවා\",\n",
        "            \"මට මේ කෑම කමතියි\",\n",
        "            \"කාර් එක රතුයි\",\n",
        "            \"ඇය ලස්සනයි\",\n",
        "            \"ඔහු උසයි\",\n",
        "            \"අපි යාළුවෝයි\",\n",
        "            \"ඔවුන් සිසුන්\",\n",
        "            \"මම පාසලට යනවා\",\n",
        "            \"මම කාර්යාලයක වැඩ කරනවා\"\n",
        "        ]\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample data:\")\n",
        "for i in range(5):\n",
        "    print(f\"English: {english_sentences[i]}\")\n",
        "    print(f\"Sinhala: {sinhala_sentences[i]}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNuhAevkxrPB",
        "outputId": "f7587513-e8bd-436e-9c15-6cf8a7deee63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vocabularies...\n",
            "English vocab size: 37\n",
            "Sinhala vocab size: 49\n",
            "Sample English vocab: ['<PAD>', '<START>', '<END>', '<UNK>', ' ', 'e', 'o', 'a', 'i', 't', 'n', 'r', 'h', 's', 'l', 'u', 'm', 'y', 'd', 'I']\n",
            "Sample Sinhala vocab: ['<PAD>', '<START>', '<END>', '<UNK>', ' ', 'න', 'ම', '්', 'ය', 'ක', 'ව', 'ි', 'ු', 'ස', 'ා', 'ට', 'ද', 'ෙ', 'ත', 'ර']\n"
          ]
        }
      ],
      "source": [
        "# Dataset class for parallel text\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, english_sentences, sinhala_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.sinhala_sentences = sinhala_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.sinhala_sentences[idx]\n",
        "\n",
        "# Improved vocabulary creation with better character handling\n",
        "def create_vocabulary(sentences, min_freq=1):\n",
        "    # Character-level tokenization with cleaning\n",
        "    char_counter = Counter()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Clean the sentence and count characters\n",
        "        cleaned_sentence = sentence.strip()\n",
        "        for char in cleaned_sentence:\n",
        "            char_counter[char] += 1\n",
        "\n",
        "    # Create vocabulary with special tokens first\n",
        "    vocab = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
        "\n",
        "    # Add characters sorted by frequency (most common first)\n",
        "    for char, freq in char_counter.most_common():\n",
        "        if freq >= min_freq and char not in vocab:\n",
        "            vocab.append(char)\n",
        "\n",
        "    # Create index mappings\n",
        "    char_to_index = {char: i for i, char in enumerate(vocab)}\n",
        "    index_to_char = {i: char for i, char in enumerate(vocab)}\n",
        "\n",
        "    return char_to_index, index_to_char, vocab\n",
        "\n",
        "# Create vocabularies\n",
        "print(\"Creating vocabularies...\")\n",
        "english_to_index, index_to_english, english_vocab = create_vocabulary(english_sentences)\n",
        "sinhala_to_index, index_to_sinhala, sinhala_vocab = create_vocabulary(sinhala_sentences)\n",
        "\n",
        "print(f\"English vocab size: {len(english_vocab)}\")\n",
        "print(f\"Sinhala vocab size: {len(sinhala_vocab)}\")\n",
        "print(f\"Sample English vocab: {english_vocab[:20]}\")\n",
        "print(f\"Sample Sinhala vocab: {sinhala_vocab[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IFQT9BXzi2z"
      },
      "source": [
        "##Initializing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfYtCXhcxrMr",
        "outputId": "006171b6-d4e8-4630-a5e4-f7da4d5bd253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total model parameters: 1,011,121\n",
            "Trainable parameters: 1,011,121\n",
            "Train samples: 32\n",
            "Validation samples: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define special tokens\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "PADDING_TOKEN = '<PAD>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "# Model hyperparameters (optimized for small dataset)\n",
        "d_model = 128\n",
        "ffn_hidden = 256\n",
        "num_heads = 4\n",
        "drop_prob = 0.1\n",
        "num_layers = 3\n",
        "max_sequence_length = 64\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "num_epochs = 10\n",
        "\n",
        "model = Transformer(\n",
        "    d_model=d_model,\n",
        "    ffn_hidden=ffn_hidden,\n",
        "    num_heads=num_heads,\n",
        "    drop_prob=drop_prob,\n",
        "    num_layers=num_layers,\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    si_vocab_size=len(sinhala_vocab),\n",
        "    english_to_index=english_to_index,\n",
        "    sinhala_to_index=sinhala_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ").to(device)\n",
        "\n",
        "# Print model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total model parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Create train/validation split\n",
        "train_size = int(0.8 * len(english_sentences))\n",
        "val_size = len(english_sentences) - train_size\n",
        "\n",
        "# Shuffle data before splitting\n",
        "combined = list(zip(english_sentences, sinhala_sentences))\n",
        "random.shuffle(combined)\n",
        "english_sentences, sinhala_sentences = zip(*combined)\n",
        "\n",
        "train_english = list(english_sentences[:train_size])\n",
        "train_sinhala = list(sinhala_sentences[:train_size])\n",
        "val_english = list(english_sentences[train_size:])\n",
        "val_sinhala = list(sinhala_sentences[train_size:])\n",
        "\n",
        "print(f\"Train samples: {len(train_english)}\")\n",
        "print(f\"Validation samples: {len(val_english)}\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TranslationDataset(train_english, train_sinhala)\n",
        "val_dataset = TranslationDataset(val_english, val_sinhala)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=sinhala_to_index[PADDING_TOKEN])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JN8uHrXUxrKV"
      },
      "outputs": [],
      "source": [
        "# Helper function to create attention masks\n",
        "def create_masks(src_len, tgt_len, device):\n",
        "    # Create decoder self-attention mask (causal mask)\n",
        "    decoder_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
        "    decoder_mask = decoder_mask.unsqueeze(0).unsqueeze(0).to(device)\n",
        "    return None, decoder_mask, None\n",
        "\n",
        "# Improved training function with better error handling\n",
        "def train_epoch_improved(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    for batch_idx, (english_batch, sinhala_batch) in enumerate(progress_bar):\n",
        "        try:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare target sequences (shift for teacher forcing)\n",
        "            target_input = [START_TOKEN + sentence for sentence in sinhala_batch]\n",
        "            target_output = [sentence + END_TOKEN for sentence in sinhala_batch]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                english_batch,\n",
        "                target_input,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            # Tokenize target output for loss calculation\n",
        "            target_indices = []\n",
        "            for sentence in target_output:\n",
        "                indices = []\n",
        "                for char in sentence:\n",
        "                    if char in sinhala_to_index:\n",
        "                        indices.append(sinhala_to_index[char])\n",
        "                    else:\n",
        "                        indices.append(sinhala_to_index[UNK_TOKEN])\n",
        "\n",
        "                # Pad or truncate to max_sequence_length\n",
        "                if len(indices) > max_sequence_length:\n",
        "                    indices = indices[:max_sequence_length]\n",
        "                else:\n",
        "                    while len(indices) < max_sequence_length:\n",
        "                        indices.append(sinhala_to_index[PADDING_TOKEN])\n",
        "                target_indices.append(indices)\n",
        "\n",
        "            target_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_tensor.reshape(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            print(f\"English batch: {english_batch}\")\n",
        "            print(f\"Sinhala batch: {sinhala_batch}\")\n",
        "            raise e\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "# Validation function\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (english_batch, sinhala_batch) in enumerate(dataloader):\n",
        "            # Prepare target sequences\n",
        "            target_input = [START_TOKEN + sentence for sentence in sinhala_batch]\n",
        "            target_output = [sentence + END_TOKEN for sentence in sinhala_batch]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                english_batch,\n",
        "                target_input,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            # Tokenize target output for loss calculation\n",
        "            target_indices = []\n",
        "            for sentence in target_output:\n",
        "                indices = [sinhala_to_index.get(char, sinhala_to_index['<UNK>']) for char in sentence]\n",
        "                # Pad or truncate to max_sequence_length\n",
        "                if len(indices) > max_sequence_length:\n",
        "                    indices = indices[:max_sequence_length]\n",
        "                else:\n",
        "                    while len(indices) < max_sequence_length:\n",
        "                        indices.append(sinhala_to_index[PADDING_TOKEN])\n",
        "                target_indices.append(indices)\n",
        "\n",
        "            target_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_tensor.reshape(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6bKsaHhzmHn"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "O_h970CDxrIB",
        "outputId": "c5a109a1-5452-4a37-9396-79c5d1107036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in batch 0: '<'\n",
            "English batch: ('How are you?', 'See you later', 'I am sad', 'They are students', 'It is raining', 'The car is red', 'Have a nice day', 'We are friends')\n",
            "Sinhala batch: ('ඔබ කෙසේද?', 'ඉදිරියෙදී හමුවෙමු', 'මට දුකයි', 'ඔවුන් සිසුන්', 'වැස්ස එනවා', 'කාර් එක රතුයි', 'හොඳ දවසක් ගත කරන්න', 'අපි යාළුවෝයි')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'<'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-413171870.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-3645992934.py\u001b[0m in \u001b[0;36mtrain_epoch_improved\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"English batch: {english_batch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sinhala batch: {sinhala_batch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-3645992934.py\u001b[0m in \u001b[0;36mtrain_epoch_improved\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             outputs = model(\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0menglish_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mtarget_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 dec_end_token=False): # x, y are batch of sentences\n\u001b[1;32m    301\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mbatch_tokenize\u001b[0;34m(self, batch, start_token, end_token)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m            \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sentence, start_token, end_token)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0msentence_word_indicies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0msentence_word_indicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0msentence_word_indicies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0msentence_word_indicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '<'"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch_improved(model, train_dataloader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation (similar improvements)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for english_batch, sinhala_batch in val_dataloader:\n",
        "            target_input = [START_TOKEN + sentence for sentence in sinhala_batch]\n",
        "            target_output = [sentence + END_TOKEN for sentence in sinhala_batch]\n",
        "\n",
        "            outputs = model(\n",
        "                english_batch,\n",
        "                target_input,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            target_indices = []\n",
        "            for sentence in target_output:\n",
        "                indices = []\n",
        "                for char in sentence:\n",
        "                    if char in sinhala_to_index:\n",
        "                        indices.append(sinhala_to_index[char])\n",
        "                    else:\n",
        "                        indices.append(sinhala_to_index[UNK_TOKEN])\n",
        "\n",
        "                if len(indices) > max_sequence_length:\n",
        "                    indices = indices[:max_sequence_length]\n",
        "                else:\n",
        "                    while len(indices) < max_sequence_length:\n",
        "                        indices.append(sinhala_to_index[PADDING_TOKEN])\n",
        "                target_indices.append(indices)\n",
        "\n",
        "            target_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_tensor.reshape(-1))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_batches += 1\n",
        "\n",
        "    val_loss = val_loss / val_batches\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'val_loss': val_loss,\n",
        "            'train_loss': train_loss,\n",
        "            'english_to_index': english_to_index,\n",
        "            'sinhala_to_index': sinhala_to_index,\n",
        "            'index_to_english': index_to_english,\n",
        "            'index_to_sinhala': index_to_sinhala,\n",
        "            'english_vocab': english_vocab,\n",
        "            'sinhala_vocab': sinhala_vocab\n",
        "        }, 'best_transformer_model.pth')\n",
        "        print(f\"New best model saved! Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch > 20 and val_loss > best_val_loss * 1.1:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Save vocabularies separately\n",
        "with open('vocabularies.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'english_to_index': english_to_index,\n",
        "        'index_to_english': index_to_english,\n",
        "        'english_vocab': english_vocab,\n",
        "        'sinhala_to_index': sinhala_to_index,\n",
        "        'index_to_sinhala': index_to_sinhala,\n",
        "        'sinhala_vocab': sinhala_vocab\n",
        "    }, f)\n",
        "\n",
        "print(\"Training completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eTi6J5xysOq"
      },
      "outputs": [],
      "source": [
        "# Translation function with beam search\n",
        "def translate_beam_search(model, english_sentence, english_to_index, sinhala_to_index, index_to_sinhala, beam_size=3, max_length=128):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Initialize beam with START token\n",
        "        beams = [(START_TOKEN, 0.0)]  # (sequence, score)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            new_beams = []\n",
        "\n",
        "            for sequence, score in beams:\n",
        "                # Check if sequence ended\n",
        "                if sequence.endswith(END_TOKEN):\n",
        "                    new_beams.append((sequence, score))\n",
        "                    continue\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    [english_sentence],\n",
        "                    [sequence],\n",
        "                    encoder_self_attention_mask=None,\n",
        "                    decoder_self_attention_mask=None,\n",
        "                    decoder_cross_attention_mask=None,\n",
        "                    enc_start_token=False,\n",
        "                    enc_end_token=False,\n",
        "                    dec_start_token=False,\n",
        "                    dec_end_token=False\n",
        "                )\n",
        "\n",
        "                # Get probabilities for next character\n",
        "                next_char_logits = outputs[0, len(sequence), :]\n",
        "                next_char_probs = torch.softmax(next_char_logits, dim=-1)\n",
        "\n",
        "                # Get top k candidates\n",
        "                top_k_probs, top_k_indices = torch.topk(next_char_probs, beam_size)\n",
        "\n",
        "                # Create new beam candidates\n",
        "                for prob, idx in zip(top_k_probs, top_k_indices):\n",
        "                    next_char = index_to_sinhala[idx.item()]\n",
        "                    new_sequence = sequence + next_char\n",
        "                    new_score = score + torch.log(prob).item()\n",
        "                    new_beams.append((new_sequence, new_score))\n",
        "\n",
        "            # Select top beam_size beams\n",
        "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "            # Check if all beams ended\n",
        "            if all(seq.endswith(END_TOKEN) for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "        # Return best sequence\n",
        "        best_sequence = beams[0][0]\n",
        "        # Remove special tokens\n",
        "        best_sequence = best_sequence.replace(START_TOKEN, '').replace(END_TOKEN, '')\n",
        "        return best_sequence\n",
        "\n",
        "# Simple greedy translation function\n",
        "def translate_greedy(model, english_sentence, english_to_index, sinhala_to_index, index_to_sinhala, max_length=128):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start with START token\n",
        "        translated = START_TOKEN\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                [english_sentence],\n",
        "                [translated],\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            # Get the next character prediction\n",
        "            next_char_logits = outputs[0, len(translated), :]\n",
        "            next_char_id = torch.argmax(next_char_logits).item()\n",
        "            next_char = index_to_sinhala[next_char_id]\n",
        "\n",
        "            # Stop if END token is generated\n",
        "            if next_char == END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # Add to translated sentence\n",
        "            translated += next_char\n",
        "\n",
        "        # Remove START token\n",
        "        translated = translated.replace(START_TOKEN, '')\n",
        "        return translated\n",
        "\n",
        "# Load the best model\n",
        "print(\"Loading best model...\")\n",
        "checkpoint = torch.load('best_transformer_model.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded model from epoch {checkpoint['epoch']} with validation loss {checkpoint['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNYIL3IVzpev"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEZjnJxoysL1"
      },
      "outputs": [],
      "source": [
        "# Test translation function\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TESTING TRANSLATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test with training samples\n",
        "test_indices = random.sample(range(len(val_english)), min(5, len(val_english)))\n",
        "\n",
        "for idx in test_indices:\n",
        "    english_text = val_english[idx]\n",
        "    expected_sinhala = val_sinhala[idx]\n",
        "\n",
        "    translation_greedy = translate_greedy(model, english_text, english_to_index, sinhala_to_index, index_to_sinhala)\n",
        "\n",
        "    print(f\"English: {english_text}\")\n",
        "    print(f\"Expected: {expected_sinhala}\")\n",
        "    print(f\"Generated: {translation_greedy}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awXJ0iqW0_iU"
      },
      "outputs": [],
      "source": [
        "# Interactive translation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INTERACTIVE TRANSLATION\")\n",
        "print(\"=\"*50)\n",
        "print(\"Enter English text to translate (type 'quit' to exit):\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nEnglish: \").strip()\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Try both translation methods\n",
        "        translation_greedy = translate_greedy(model, user_input, english_to_index, sinhala_to_index, index_to_sinhala)\n",
        "        print(f\"Sinhala (Greedy): {translation_greedy}\")\n",
        "\n",
        "        # Uncomment the line below for beam search (slower but potentially better)\n",
        "        # translation_beam = translate_beam_search(model, user_input, english_to_index, sinhala_to_index, index_to_sinhala)\n",
        "        # print(f\"Sinhala (Beam): {translation_beam}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"This might happen if the input contains characters not in the vocabulary.\")\n",
        "\n",
        "print(\"Translation session ended!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOFTMx7W1Bc8"
      },
      "outputs": [],
      "source": [
        "# Save final model info\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total epochs: {num_epochs}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"English vocabulary size: {len(english_vocab)}\")\n",
        "print(f\"Sinhala vocabulary size: {len(sinhala_vocab)}\")\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOfoBS1va/avHQ4UuiPL6aj",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03370dcb5262415bbe8e312f933c6805": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0b67d37cc6924b22b0524353c8773e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3799132bc11c4488bd2e73a7ab1fdd47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3824fd9551104011b103d0772fb1b6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ec26f5619fe4b76a83256ad9c2f3c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d59c3538c7e4954989d67046a0b0fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523671f1674a45a3bbee1fb69003a174": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d59c3538c7e4954989d67046a0b0fc2",
            "placeholder": "​",
            "style": "IPY_MODEL_3824fd9551104011b103d0772fb1b6ce",
            "value": "Downloading readme: "
          }
        },
        "5de21bea4bb24538b5a07c905df753f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_523671f1674a45a3bbee1fb69003a174",
              "IPY_MODEL_de2d632ab64f4db1a1334823c8f6f2da",
              "IPY_MODEL_8812cf8b23614cc190f0279566f6a180"
            ],
            "layout": "IPY_MODEL_0b67d37cc6924b22b0524353c8773e3c"
          }
        },
        "8812cf8b23614cc190f0279566f6a180": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_affc57d1185a43c68330cd6a6857f102",
            "placeholder": "​",
            "style": "IPY_MODEL_3ec26f5619fe4b76a83256ad9c2f3c8d",
            "value": " 1.92k/? [00:00&lt;00:00, 31.0kB/s]"
          }
        },
        "affc57d1185a43c68330cd6a6857f102": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2d632ab64f4db1a1334823c8f6f2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03370dcb5262415bbe8e312f933c6805",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3799132bc11c4488bd2e73a7ab1fdd47",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
