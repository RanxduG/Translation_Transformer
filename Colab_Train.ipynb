{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RanxduG/Translation_Transformer/blob/main/Colab_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEA_Mw3TzH-f"
      },
      "source": [
        "##Install required packages testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4d-cbkmol7P",
        "outputId": "eb7f3d26-9772-4236-c309-53f3803ce3e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers datasets\n",
        "!pip install matplotlib seaborn\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMRpmFrizL0R"
      },
      "source": [
        "##Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jL_OEKwIxrWl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from transformer import Transformer, get_device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oaBBUFlEtYr",
        "outputId": "43ed9b07-50dd-4677-96cc-b099278ca03a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIL0GMfhzP1E"
      },
      "source": [
        "##Initializing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWdFSYG6xrTu",
        "outputId": "2cee8ba8-59b9-44e3-d872-f8d5ddd09679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load English-Sinhala translation datasets...\n",
            "\n",
            "1. Trying OPUS-100 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✗ OPUS-100 failed: Invalid pattern: '**' can only be an entire path component\n",
            "\n",
            "2. Trying alternative OPUS-100 loading method...\n",
            "✗ Alternative OPUS-100 failed: Invalid pattern: '**' can only be an entire path component\n",
            "\n",
            "3. Trying NLPC-UOM sentence alignment dataset...\n",
            "✗ NLPC-UOM failed: Invalid pattern: '**' can only be an entire path component\n",
            "\n",
            "4. Trying to download from Tatoeba Challenge dataset...\n",
            "✓ Using comprehensive sample dataset with 181 sentence pairs\n",
            "\n",
            "Validating dataset with 181 sentence pairs...\n",
            "After cleaning: 181 valid sentence pairs\n",
            "\n",
            "Dataset is small (181 pairs). Expanding with variations...\n",
            "Expanded dataset to 342 sentence pairs\n",
            "Dataset saved as english_sinhala_dataset.csv and english_sinhala_dataset.json\n",
            "\n",
            "Sample data from the final dataset:\n",
            "English: Hello\n",
            "Sinhala: ආයුබෝවන්\n",
            "---\n",
            "English: Hi\n",
            "Sinhala: හායි\n",
            "---\n",
            "English: Good morning\n",
            "Sinhala: සුභ උදෑසනක්\n",
            "---\n",
            "English: Good afternoon\n",
            "Sinhala: සුභ මධ්‍යාහ්නයක්\n",
            "---\n",
            "English: Good evening\n",
            "Sinhala: සුභ සන්ධ්‍යාවක්\n",
            "---\n",
            "English: Good night\n",
            "Sinhala: සුභ රාත්‍රියක්\n",
            "---\n",
            "English: How are you?\n",
            "Sinhala: ඔබ කෙසේද?\n",
            "---\n",
            "English: I am fine\n",
            "Sinhala: මම හොඳයි\n",
            "---\n",
            "English: Thank you\n",
            "Sinhala: ස්තූතියි\n",
            "---\n",
            "English: You are welcome\n",
            "Sinhala: ඔබට ස්තූතියි\n",
            "---\n",
            "\n",
            "Final dataset ready with 342 sentence pairs!\n",
            "You can now use this data to train your transformer model.\n",
            "\n",
            "To use this data in your transformer training, you can access:\n",
            "- english_sentences: List of English sentences\n",
            "- sinhala_sentences: List of corresponding Sinhala sentences\n"
          ]
        }
      ],
      "source": [
        "def download_and_prepare_data():\n",
        "    \"\"\"\n",
        "    Try multiple dataset sources and prepare English-Sinhala translation data\n",
        "    \"\"\"\n",
        "    english_sentences = []\n",
        "    sinhala_sentences = []\n",
        "\n",
        "    print(\"Attempting to load English-Sinhala translation datasets...\")\n",
        "\n",
        "    # Method 1: Try OPUS-100 dataset (most reliable)\n",
        "    print(\"\\n1. Trying OPUS-100 dataset...\")\n",
        "    try:\n",
        "        # Load OPUS-100 English-Sinhala\n",
        "        dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-si\", split=\"train\")\n",
        "\n",
        "        # Extract sentences\n",
        "        for example in dataset:\n",
        "            if len(example['translation']['en'].strip()) > 0 and len(example['translation']['si'].strip()) > 0:\n",
        "                english_sentences.append(example['translation']['en'].strip())\n",
        "                sinhala_sentences.append(example['translation']['si'].strip())\n",
        "\n",
        "                # Limit to reasonable size for training\n",
        "                if len(english_sentences) >= 50000:\n",
        "                    break\n",
        "\n",
        "        print(f\"✓ Successfully loaded {len(english_sentences)} sentence pairs from OPUS-100\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ OPUS-100 failed: {e}\")\n",
        "\n",
        "        # Method 2: Try Alternative OPUS-100 approach\n",
        "        print(\"\\n2. Trying alternative OPUS-100 loading method...\")\n",
        "        try:\n",
        "            dataset = load_dataset(\"opus100\", \"en-si\")\n",
        "\n",
        "            for split in ['train', 'validation', 'test']:\n",
        "                if split in dataset:\n",
        "                    for example in dataset[split]:\n",
        "                        if len(example['translation']['en'].strip()) > 0 and len(example['translation']['si'].strip()) > 0:\n",
        "                            english_sentences.append(example['translation']['en'].strip())\n",
        "                            sinhala_sentences.append(example['translation']['si'].strip())\n",
        "\n",
        "                            if len(english_sentences) >= 50000:\n",
        "                                break\n",
        "                    if len(english_sentences) >= 50000:\n",
        "                        break\n",
        "\n",
        "            print(f\"✓ Successfully loaded {len(english_sentences)} sentence pairs from OPUS-100 (alt method)\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"✗ Alternative OPUS-100 failed: {e2}\")\n",
        "\n",
        "            # Method 3: Try NLPC-UOM dataset with different approach\n",
        "            print(\"\\n3. Trying NLPC-UOM sentence alignment dataset...\")\n",
        "            try:\n",
        "                # Try to load the dataset with specific configuration\n",
        "                dataset = load_dataset(\"NLPC-UOM/sentence_alignment_dataset-Sinhala-Tamil-English\",\n",
        "                                     trust_remote_code=True)\n",
        "\n",
        "                # Process the data\n",
        "                for split_name in dataset.keys():\n",
        "                    split_data = dataset[split_name]\n",
        "                    for example in split_data:\n",
        "                        # Check different possible field names\n",
        "                        english_text = None\n",
        "                        sinhala_text = None\n",
        "\n",
        "                        # Try different field combinations\n",
        "                        if 'english' in example and 'sinhala' in example:\n",
        "                            english_text = example['english']\n",
        "                            sinhala_text = example['sinhala']\n",
        "                        elif 'en' in example and 'si' in example:\n",
        "                            english_text = example['en']\n",
        "                            sinhala_text = example['si']\n",
        "                        elif 'source' in example and 'target' in example:\n",
        "                            english_text = example['source']\n",
        "                            sinhala_text = example['target']\n",
        "\n",
        "                        if english_text and sinhala_text:\n",
        "                            if len(english_text.strip()) > 0 and len(sinhala_text.strip()) > 0:\n",
        "                                english_sentences.append(english_text.strip())\n",
        "                                sinhala_sentences.append(sinhala_text.strip())\n",
        "\n",
        "                                if len(english_sentences) >= 20000:\n",
        "                                    break\n",
        "\n",
        "                    if len(english_sentences) >= 20000:\n",
        "                        break\n",
        "\n",
        "                print(f\"✓ Successfully loaded {len(english_sentences)} sentence pairs from NLPC-UOM\")\n",
        "\n",
        "            except Exception as e3:\n",
        "                print(f\"✗ NLPC-UOM failed: {e3}\")\n",
        "\n",
        "                # Method 4: Try downloading from a direct source\n",
        "                print(\"\\n4. Trying to download from Tatoeba Challenge dataset...\")\n",
        "                try:\n",
        "                    # This is a backup method using publicly available data\n",
        "                    # You can replace this URL with any publicly available English-Sinhala dataset\n",
        "\n",
        "                    # For now, let's create a more comprehensive sample dataset\n",
        "                    english_sentences, sinhala_sentences = create_comprehensive_sample_dataset()\n",
        "                    print(f\"✓ Using comprehensive sample dataset with {len(english_sentences)} sentence pairs\")\n",
        "\n",
        "                except Exception as e4:\n",
        "                    print(f\"✗ Direct download failed: {e4}\")\n",
        "\n",
        "                    # Method 5: Fallback to basic sample\n",
        "                    print(\"\\n5. Using basic sample dataset...\")\n",
        "                    english_sentences, sinhala_sentences = create_basic_sample_dataset()\n",
        "                    print(f\"✓ Using basic sample dataset with {len(english_sentences)} sentence pairs\")\n",
        "\n",
        "    return english_sentences, sinhala_sentences\n",
        "\n",
        "def create_comprehensive_sample_dataset():\n",
        "    \"\"\"\n",
        "    Create a more comprehensive sample dataset for training\n",
        "    \"\"\"\n",
        "    english_sentences = [\n",
        "        # Basic greetings and common phrases\n",
        "        \"Hello\", \"Hi\", \"Good morning\", \"Good afternoon\", \"Good evening\", \"Good night\",\n",
        "        \"How are you?\", \"I am fine\", \"Thank you\", \"You are welcome\", \"Please\", \"Excuse me\",\n",
        "        \"Sorry\", \"Goodbye\", \"See you later\", \"Have a nice day\", \"Nice to meet you\",\n",
        "\n",
        "        # Personal information\n",
        "        \"What is your name?\", \"My name is John\", \"How old are you?\", \"I am 25 years old\",\n",
        "        \"Where are you from?\", \"I am from Sri Lanka\", \"I am from America\", \"I live in Colombo\",\n",
        "        \"What do you do?\", \"I am a student\", \"I am a teacher\", \"I work in an office\",\n",
        "\n",
        "        # Family and relationships\n",
        "        \"This is my family\", \"I have a brother\", \"I have a sister\", \"My father is a doctor\",\n",
        "        \"My mother is a teacher\", \"I love my family\", \"We are friends\", \"He is my friend\",\n",
        "        \"She is my colleague\", \"They are my parents\",\n",
        "\n",
        "        # Daily activities\n",
        "        \"I wake up at 6 AM\", \"I go to work\", \"I come home\", \"I eat breakfast\",\n",
        "        \"I eat lunch\", \"I eat dinner\", \"I go to sleep\", \"I watch TV\", \"I read books\",\n",
        "        \"I listen to music\", \"I play games\", \"I exercise\", \"I go shopping\",\n",
        "\n",
        "        # Food and drinks\n",
        "        \"I am hungry\", \"I am thirsty\", \"I want to eat\", \"I want to drink water\",\n",
        "        \"The food is delicious\", \"I like rice\", \"I like curry\", \"I drink tea\",\n",
        "        \"I drink coffee\", \"This is spicy\", \"This is sweet\", \"I am full\",\n",
        "\n",
        "        # Weather and time\n",
        "        \"What time is it?\", \"It is 3 o'clock\", \"Today is Monday\", \"Tomorrow is Tuesday\",\n",
        "        \"It is sunny\", \"It is raining\", \"It is hot\", \"It is cold\", \"The weather is nice\",\n",
        "        \"I like this weather\", \"What day is today?\", \"What is the date?\",\n",
        "\n",
        "        # Places and directions\n",
        "        \"Where is the hospital?\", \"Where is the school?\", \"Where is the market?\",\n",
        "        \"Go straight\", \"Turn left\", \"Turn right\", \"It is near here\", \"It is far from here\",\n",
        "        \"I want to go to the beach\", \"I want to go to the mountains\", \"I live in the city\",\n",
        "\n",
        "        # Shopping and money\n",
        "        \"How much does it cost?\", \"It is expensive\", \"It is cheap\", \"I want to buy this\",\n",
        "        \"Do you have change?\", \"I need money\", \"Where is the bank?\", \"I want to pay\",\n",
        "        \"Can I have a discount?\", \"This is too expensive\", \"I will take it\",\n",
        "\n",
        "        # Transportation\n",
        "        \"I go by bus\", \"I go by car\", \"I go by train\", \"I walk\", \"I ride a bicycle\",\n",
        "        \"Where is the bus stop?\", \"When does the bus come?\", \"I missed the bus\",\n",
        "        \"Call a taxi\", \"I need to go to the airport\", \"How long does it take?\",\n",
        "\n",
        "        # Education and work\n",
        "        \"I go to school\", \"I study hard\", \"I have an exam\", \"I passed the exam\",\n",
        "        \"I failed the exam\", \"I graduated\", \"I work hard\", \"I am busy\", \"I am free\",\n",
        "        \"I have a meeting\", \"I finished my work\", \"I am tired\",\n",
        "\n",
        "        # Health and feelings\n",
        "        \"I am sick\", \"I feel better\", \"I have a headache\", \"I have a fever\",\n",
        "        \"I need to see a doctor\", \"I am happy\", \"I am sad\", \"I am angry\",\n",
        "        \"I am excited\", \"I am worried\", \"I am surprised\", \"I am confused\",\n",
        "\n",
        "        # Technology and communication\n",
        "        \"I have a phone\", \"I use a computer\", \"I send an email\", \"I watch videos\",\n",
        "        \"I use the internet\", \"My phone is broken\", \"I need to charge my phone\",\n",
        "        \"Can you call me?\", \"I will call you later\", \"Send me a message\",\n",
        "\n",
        "        # Objects and descriptions\n",
        "        \"The book is on the table\", \"The car is red\", \"The house is big\",\n",
        "        \"The room is small\", \"The food is hot\", \"The water is cold\",\n",
        "        \"She is beautiful\", \"He is tall\", \"It is new\", \"It is old\",\n",
        "        \"This is good\", \"This is bad\", \"It is clean\", \"It is dirty\",\n",
        "\n",
        "        # Actions and verbs\n",
        "        \"I can swim\", \"I can sing\", \"I can dance\", \"I can cook\", \"I can drive\",\n",
        "        \"I cannot speak Sinhala well\", \"I am learning\", \"I understand\", \"I don't understand\",\n",
        "        \"Can you help me?\", \"I will help you\", \"Let's go\", \"Wait for me\", \"Hurry up\",\n",
        "\n",
        "        # Numbers and counting\n",
        "        \"I have one book\", \"I have two sisters\", \"I have three friends\",\n",
        "        \"I need four chairs\", \"I bought five apples\", \"There are six people\",\n",
        "        \"I have seven days\", \"I work eight hours\", \"I sleep nine hours\", \"I have ten fingers\"\n",
        "    ]\n",
        "\n",
        "    sinhala_sentences = [\n",
        "        # Basic greetings and common phrases\n",
        "        \"ආයුබෝවන්\", \"හායි\", \"සුභ උදෑසනක්\", \"සුභ මධ්‍යාහ්නයක්\", \"සුභ සන්ධ්‍යාවක්\", \"සුභ රාත්‍රියක්\",\n",
        "        \"ඔබ කෙසේද?\", \"මම හොඳයි\", \"ස්තූතියි\", \"ඔබට ස්තූතියි\", \"කරුණාකර\", \"සමාවන්න\",\n",
        "        \"සමාවන්න\", \"ආයුබෝවන්\", \"ඉදිරියෙදී හමුවෙමු\", \"හොඳ දවසක් ගත කරන්න\", \"ඔබව හමුවීමට සතුටුයි\",\n",
        "\n",
        "        # Personal information\n",
        "        \"ඔබගේ නම මොකක්ද?\", \"මගේ නම ජෝන්\", \"ඔබට වයස කීයද?\", \"මට වයස 25යි\",\n",
        "        \"ඔබ කොහෙන්ද?\", \"මම ශ්‍රී ලංකාවෙන්\", \"මම ඇමරිකාවෙන්\", \"මම කොළඹ ජීවත් වෙනවා\",\n",
        "        \"ඔබ මොනවා කරනවාද?\", \"මම ශිෂ්‍යයෙක්\", \"මම ගුරුවරයෙක්\", \"මම කාර්යාලයක වැඩ කරනවා\",\n",
        "\n",
        "        # Family and relationships\n",
        "        \"මේ මගේ පවුල\", \"මට සහෝදරයෙක් ඉන්නවා\", \"මට සහෝදරියක් ඉන්නවා\", \"මගේ තාත්තා වෛද්‍යවරයෙක්\",\n",
        "        \"මගේ අම්මා ගුරුවරියක්\", \"මම මගේ පවුලට ආදරෙයි\", \"අපි යාළුවෝයි\", \"ඔහු මගේ යාළුවා\",\n",
        "        \"ඇය මගේ සහකාරිය\", \"ඔවුන් මගේ දෙමාපියන්\",\n",
        "\n",
        "        # Daily activities\n",
        "        \"මම උදේ 6ට අවදි වෙනවා\", \"මම වැඩට යනවා\", \"මම ගෙදර එනවා\", \"මම උදෑසන කෑම කනවා\",\n",
        "        \"මම දිවා කෑම කනවා\", \"මම රාත්‍රී කෑම කනවා\", \"මම නිදාගන්නවා\", \"මම ටීවී බලනවා\", \"මම පොත් කියවනවා\",\n",
        "        \"මම සංගීතය අහනවා\", \"මම ක්‍රීඩා කරනවා\", \"මම ව්‍යායාම කරනවා\", \"මම සාප්පු යනවා\",\n",
        "\n",
        "        # Food and drinks\n",
        "        \"මට කිඩෙන් හිතෙනවා\", \"මට තිරගැන්මක් හිතෙනවා\", \"මට කන්න ඕන\", \"මට වතුර බොන්න ඕන\",\n",
        "        \"කෑම රසයි\", \"මට බත් කමතියි\", \"මට කරි කමතියි\", \"මම තේ බොනවා\",\n",
        "        \"මම කෝපි බොනවා\", \"මේක ඇඹුල්\", \"මේක මිහිරි\", \"මම සම්පූර්ණයි\",\n",
        "\n",
        "        # Weather and time\n",
        "        \"වේලාව කීයද?\", \"වේලාව 3යි\", \"අද සඳුදා\", \"හෙට අඟහරුවාදා\",\n",
        "        \"හිරු එළියෙන් ඉන්නවා\", \"වැස්ස එනවා\", \"රස්නෙයි\", \"සීතලයි\", \"කාලගුණය හොඳයි\",\n",
        "        \"මට මේ කාලගුණය කමතියි\", \"අද කුමන දවසද?\", \"අද දිනය මොකක්ද?\",\n",
        "\n",
        "        # Places and directions\n",
        "        \"රෝහල කොහෙද?\", \"පාසල කොහෙද?\", \"පොළ කොහෙද?\",\n",
        "        \"කෙලින් යන්න\", \"වමට හරවන්න\", \"දකුණට හරවන්න\", \"එය මෙතන ලඟයි\", \"එය මෙතන ඈතයි\",\n",
        "        \"මට මුහුදු තීරයට යන්න ඕන\", \"මට කන්දට යන්න ඕන\", \"මම නගරයේ ජීවත් වෙනවා\",\n",
        "\n",
        "        # Shopping and money\n",
        "        \"මේක කීයද?\", \"එය මිල අධිකයි\", \"එය මිල අඩුයි\", \"මට මේක ගන්න ඕන\",\n",
        "        \"ඔබ ළඟ සුදුවක් තියෙනවාද?\", \"මට සල්ලි ඕන\", \"බැංකුව කොහෙද?\", \"මට ගෙවන්න ඕන\",\n",
        "        \"මට වට්ටමක් ලබා ගන්න පුළුවන්ද?\", \"මේක හරිම මිල අධිකයි\", \"මම මේක ගන්නම්\",\n",
        "\n",
        "        # Transportation\n",
        "        \"මම බස් එකේ යනවා\", \"මම කාර් එකේ යනවා\", \"මම කෝච්චියේ යනවා\", \"මම පයින් යනවා\", \"මම පාපැදියේ යනවා\",\n",
        "        \"බස් නැවතුම කොහෙද?\", \"බස් එක කවදාද එන්නෙ?\", \"මම බස් එක මග හැරුණා\",\n",
        "        \"කැබ් එකක් කෝල් කරන්න\", \"මට ගුවන්තොටුපළට යන්න ඕන\", \"කාලය කීයක් ගන්නවාද?\",\n",
        "\n",
        "        # Education and work\n",
        "        \"මම පාසලට යනවා\", \"මම මහන්සියෙන් ඉගෙන ගන්නවා\", \"මට විභාගයක් තියෙනවා\", \"මම විභාගයෙන් පාස් උණා\",\n",
        "        \"මම විභාගයෙන් ෆේල් උණා\", \"මම උපාධිය ගත්තා\", \"මම මහන්සියෙන් වැඩ කරනවා\", \"මම කාර්යබහුලයි\", \"මම නිදහස්\",\n",
        "        \"මට රැස්වීමක් තියෙනවා\", \"මම මගේ වැඩ ඉවර කළා\", \"මට කඩාවැටෙනවා\",\n",
        "\n",
        "        # Health and feelings\n",
        "        \"මම අසනීපයි\", \"මට සනීප හිතෙනවා\", \"මට හිස රිදෙනවා\", \"මට උණ\",\n",
        "        \"මට වෛද්‍යවරයෙක් දකින්න ඕන\", \"මට සතුටුයි\", \"මට දුකයි\", \"මට කේන්තියි\",\n",
        "        \"මම උද්‍යෝගිමත්\", \"මම කනස්සල්ලෙන්\", \"මම පුදුම හිතෙන්\", \"මම ව්‍යාකූල\",\n",
        "\n",
        "        # Technology and communication\n",
        "        \"මට ෆෝන් එකක් තියෙනවා\", \"මම පරිගණකයක් පාවිච්චි කරනවා\", \"මම ඊමේල් එකක් එවනවා\", \"මම වීඩියෝ බලනවා\",\n",
        "        \"මම අන්තර්ජාලය පාවිච්චි කරනවා\", \"මගේ ෆෝන් එක කැඩුණා\", \"මට මගේ ෆෝන් එක චාජ් කරන්න ඕන\",\n",
        "        \"ඔබට මට කෝල් කරන්න පුළුවන්ද?\", \"මම ඔබට පස්සේ කෝල් කරන්නම්\", \"මට පණිවිඩයක් එවන්න\",\n",
        "\n",
        "        # Objects and descriptions\n",
        "        \"පොත මේසය උඩින් තියෙනවා\", \"කාර් එක රතුයි\", \"ගෙදර විශාලයි\",\n",
        "        \"කාමරය කුඩායි\", \"කෑම උණුසුම්\", \"වතුර සීතලයි\",\n",
        "        \"ඇය ලස්සනයි\", \"ඔහු උසයි\", \"එය අලුත්\", \"එය පරණයි\",\n",
        "        \"මේක හොඳයි\", \"මේක නරකයි\", \"එය පිරිසිදුයි\", \"එය අපිරිසිදුයි\",\n",
        "\n",
        "        # Actions and verbs\n",
        "        \"මට පිහිනනවා පුළුවන්\", \"මට කියන්න පුළුවන්\", \"මට නටන්න පුළුවන්\", \"මට උයන්න පුළුවන්\", \"මට ධාවනය කරන්න පුළුවන්\",\n",
        "        \"මට සිංහල හොඳට කතා කරන්න බෑ\", \"මම ඉගෙන ගන්නම්\", \"මට තේරෙනවා\", \"මට තේරෙන්නේ නෑ\",\n",
        "        \"ඔබට මට උදව් කරන්න පුළුවන්ද?\", \"මම ඔබට උදව් කරන්නම්\", \"අපි යමු\", \"මට ඉන්න දෙන්න\", \"ඉක්මන් කරන්න\",\n",
        "\n",
        "        # Numbers and counting\n",
        "        \"මට පොත්තක් තියෙනවා\", \"මට සහෝදරියන් දෙන්නෙක් ඉන්නවා\", \"මට යාළුවෝ තුන්දෙනෙක් ඉන්නවා\",\n",
        "        \"මට පුටු හතරක් ඕන\", \"මම ඇපල් පහක් ගත්තා\", \"මනුෂ්‍යයෝ හය්දෙනෙක් ඉන්නවා\",\n",
        "        \"මට දින හතක් තියෙනවා\", \"මම පැය අටක් වැඩ කරනවා\", \"මම පැය නවයක් නිදාගන්නවා\", \"මට ඇඟිලි දහයක් තියෙනවා\"\n",
        "    ]\n",
        "\n",
        "    return english_sentences, sinhala_sentences\n",
        "\n",
        "def create_basic_sample_dataset():\n",
        "    \"\"\"\n",
        "    Create a basic sample dataset as final fallback\n",
        "    \"\"\"\n",
        "    english_sentences = [\n",
        "        \"Hello\", \"How are you?\", \"Good morning\", \"Good evening\", \"Thank you\",\n",
        "        \"You are welcome\", \"I am fine\", \"What is your name?\", \"Nice to meet you\",\n",
        "        \"How old are you?\", \"Where are you from?\", \"I love you\", \"Goodbye\",\n",
        "        \"See you later\", \"Have a nice day\", \"I am hungry\", \"I am tired\",\n",
        "        \"What time is it?\", \"It is raining\", \"The weather is nice\"\n",
        "    ]\n",
        "\n",
        "    sinhala_sentences = [\n",
        "        \"ආයුබෝවන්\", \"ඔබ කෙසේද?\", \"සුභ උදෑසනක්\", \"සුභ සන්ධ්‍යාවක්\", \"ස්තූතියි\",\n",
        "        \"ඔබට ස්තූතියි\", \"මම හොඳයි\", \"ඔබගේ නම මොකක්ද?\", \"ඔබව හමුවීමට සතුටුයි\",\n",
        "        \"ඔබට වයස කීයද?\", \"ඔබ කොහෙන්ද?\", \"මම ඔබට ආදරෙයි\", \"ආයුබෝවන්\",\n",
        "        \"ඉදිරියෙදී හමුවෙමු\", \"හොඳ දවසක් ගත කරන්න\", \"මට කිඩෙන් හිතෙනවා\", \"මට කඩාවැටෙනවා\",\n",
        "        \"වේලාව කීයද?\", \"වැස්ස එනවා\", \"කාලගුණය හොඳයි\"\n",
        "    ]\n",
        "\n",
        "    return english_sentences, sinhala_sentences\n",
        "\n",
        "def validate_and_clean_data(english_sentences, sinhala_sentences):\n",
        "    \"\"\"\n",
        "    Validate and clean the dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\nValidating dataset with {len(english_sentences)} sentence pairs...\")\n",
        "\n",
        "    # Remove empty pairs and pairs that are too short or too long\n",
        "    cleaned_english = []\n",
        "    cleaned_sinhala = []\n",
        "\n",
        "    for en, si in zip(english_sentences, sinhala_sentences):\n",
        "        en = en.strip()\n",
        "        si = si.strip()\n",
        "\n",
        "        # Skip if either sentence is empty\n",
        "        if not en or not si:\n",
        "            continue\n",
        "\n",
        "        # Skip if sentences are too short (less than 2 characters)\n",
        "        if len(en) < 2 or len(si) < 2:\n",
        "            continue\n",
        "\n",
        "        # Skip if sentences are too long (more than 200 characters)\n",
        "        if len(en) > 200 or len(si) > 200:\n",
        "            continue\n",
        "\n",
        "        cleaned_english.append(en)\n",
        "        cleaned_sinhala.append(si)\n",
        "\n",
        "    print(f\"After cleaning: {len(cleaned_english)} valid sentence pairs\")\n",
        "    return cleaned_english, cleaned_sinhala\n",
        "\n",
        "def save_dataset(english_sentences, sinhala_sentences, filename=\"english_sinhala_dataset\"):\n",
        "    \"\"\"\n",
        "    Save the dataset to files for later use\n",
        "    \"\"\"\n",
        "    # Create a pandas DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'english': english_sentences,\n",
        "        'sinhala': sinhala_sentences\n",
        "    })\n",
        "\n",
        "    # Save as CSV\n",
        "    df.to_csv(f\"{filename}.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "    # Save as JSON\n",
        "    data = {\n",
        "        'english': english_sentences,\n",
        "        'sinhala': sinhala_sentences,\n",
        "        'count': len(english_sentences)\n",
        "    }\n",
        "\n",
        "    with open(f\"{filename}.json\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Dataset saved as {filename}.csv and {filename}.json\")\n",
        "\n",
        "\n",
        "# Download and prepare the dataset\n",
        "english_sentences, sinhala_sentences = download_and_prepare_data()\n",
        "\n",
        "# Validate and clean the data\n",
        "english_sentences, sinhala_sentences = validate_and_clean_data(english_sentences, sinhala_sentences)\n",
        "\n",
        "# If we have a small dataset, duplicate it with some variations to make it larger\n",
        "if len(english_sentences) < 1000:\n",
        "    print(f\"\\nDataset is small ({len(english_sentences)} pairs). Expanding with variations...\")\n",
        "\n",
        "    # Create variations by adding punctuation, changing case, etc.\n",
        "    expanded_english = english_sentences[:]\n",
        "    expanded_sinhala = sinhala_sentences[:]\n",
        "\n",
        "    # Add variations\n",
        "    for en, si in zip(english_sentences, sinhala_sentences):\n",
        "        # Add punctuation variations\n",
        "        if not en.endswith('.') and not en.endswith('?') and not en.endswith('!'):\n",
        "            expanded_english.append(en + '.')\n",
        "            expanded_sinhala.append(si + '.')\n",
        "\n",
        "        # Add variations with different cases\n",
        "        if en.islower():\n",
        "            expanded_english.append(en.capitalize())\n",
        "            expanded_sinhala.append(si)\n",
        "\n",
        "    english_sentences = expanded_english\n",
        "    sinhala_sentences = expanded_sinhala\n",
        "\n",
        "    print(f\"Expanded dataset to {len(english_sentences)} sentence pairs\")\n",
        "\n",
        "# Save the dataset\n",
        "save_dataset(english_sentences, sinhala_sentences)\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample data from the final dataset:\")\n",
        "sample_size = min(10, len(english_sentences))\n",
        "for i in range(sample_size):\n",
        "    print(f\"English: {english_sentences[i]}\")\n",
        "    print(f\"Sinhala: {sinhala_sentences[i]}\")\n",
        "    print(\"---\")\n",
        "\n",
        "print(f\"\\nFinal dataset ready with {len(english_sentences)} sentence pairs!\")\n",
        "print(\"You can now use this data to train your transformer model.\")\n",
        "\n",
        "# Return the data for use in your transformer training\n",
        "print(\"\\nTo use this data in your transformer training, you can access:\")\n",
        "print(\"- english_sentences: List of English sentences\")\n",
        "print(\"- sinhala_sentences: List of corresponding Sinhala sentences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNuhAevkxrPB",
        "outputId": "fbac6f49-4846-4317-ba13-782caa5a6488"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vocabularies...\n",
            "English vocab size: 54\n",
            "Sinhala vocab size: 66\n",
            "Sample English vocab: ['<PAD>', '<START>', '<END>', '<UNK>', '<', '>', ' ', 'e', 'a', 'o', 'i', 't', 's', 'I', 'r', 'n', 'h', '.', 'd', 'm']\n",
            "Sample Sinhala vocab: ['<PAD>', '<START>', '<END>', '<UNK>', '<', '>', ' ', 'ම', 'න', '්', 'ය', 'ව', 'ක', 'ි', 'ා', '.', 'ු', 'ර', 'ට', 'ත']\n"
          ]
        }
      ],
      "source": [
        "# Fixed Dataset class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, english_sentences, sinhala_sentences):\n",
        "        # Ensure inputs are lists\n",
        "        if not isinstance(english_sentences, list):\n",
        "            english_sentences = list(english_sentences)\n",
        "        if not isinstance(sinhala_sentences, list):\n",
        "            sinhala_sentences = list(sinhala_sentences)\n",
        "\n",
        "        self.english_sentences = english_sentences\n",
        "        self.sinhala_sentences = sinhala_sentences\n",
        "\n",
        "        # Validate that both lists have the same length\n",
        "        assert len(self.english_sentences) == len(self.sinhala_sentences), \\\n",
        "            f\"Mismatch in sentence counts: {len(self.english_sentences)} vs {len(self.sinhala_sentences)}\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Ensure idx is an integer\n",
        "        if isinstance(idx, (list, tuple)):\n",
        "            idx = idx[0]\n",
        "\n",
        "        # Validate index\n",
        "        if idx >= len(self.english_sentences) or idx < 0:\n",
        "            raise IndexError(f\"Index {idx} out of range for dataset of size {len(self.english_sentences)}\")\n",
        "\n",
        "        return self.english_sentences[idx], self.sinhala_sentences[idx]\n",
        "\n",
        "def create_vocabulary(sentences, min_freq=1):\n",
        "    # Character-level tokenization with cleaning\n",
        "    char_counter = Counter()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Clean the sentence and count characters\n",
        "        cleaned_sentence = sentence.strip()\n",
        "        for char in cleaned_sentence:\n",
        "            char_counter[char] += 1\n",
        "\n",
        "    # Create vocabulary with special tokens first\n",
        "    vocab = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
        "\n",
        "    # Add special characters that might appear in special tokens\n",
        "    special_chars = ['<', '>']\n",
        "    for char in special_chars:\n",
        "        if char not in vocab:\n",
        "            vocab.append(char)\n",
        "\n",
        "    # Add characters sorted by frequency (most common first)\n",
        "    for char, freq in char_counter.most_common():\n",
        "        if freq >= min_freq and char not in vocab:\n",
        "            vocab.append(char)\n",
        "\n",
        "    # Create index mappings\n",
        "    char_to_index = {char: i for i, char in enumerate(vocab)}\n",
        "    index_to_char = {i: char for i, char in enumerate(vocab)}\n",
        "\n",
        "    return char_to_index, index_to_char, vocab\n",
        "\n",
        "\n",
        "# Create vocabularies\n",
        "print(\"Creating vocabularies...\")\n",
        "english_to_index, index_to_english, english_vocab = create_vocabulary(english_sentences)\n",
        "sinhala_to_index, index_to_sinhala, sinhala_vocab = create_vocabulary(sinhala_sentences)\n",
        "\n",
        "print(f\"English vocab size: {len(english_vocab)}\")\n",
        "print(f\"Sinhala vocab size: {len(sinhala_vocab)}\")\n",
        "print(f\"Sample English vocab: {english_vocab[:20]}\")\n",
        "print(f\"Sample Sinhala vocab: {sinhala_vocab[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IFQT9BXzi2z"
      },
      "source": [
        "##Initializing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfYtCXhcxrMr",
        "outputId": "cba0f322-8b77-4841-e6ce-54d8719c9afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total model parameters: 1,017,666\n",
            "Trainable parameters: 1,017,666\n",
            "Train samples: 273\n",
            "Validation samples: 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define special tokens\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "PADDING_TOKEN = '<PAD>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "# Model hyperparameters (optimized for small dataset)\n",
        "d_model = 128\n",
        "ffn_hidden = 256\n",
        "num_heads = 4\n",
        "drop_prob = 0.1\n",
        "num_layers = 3\n",
        "max_sequence_length = 64\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "num_epochs = 100\n",
        "\n",
        "model = Transformer(\n",
        "    d_model=d_model,\n",
        "    ffn_hidden=ffn_hidden,\n",
        "    num_heads=num_heads,\n",
        "    drop_prob=drop_prob,\n",
        "    num_layers=num_layers,\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    si_vocab_size=len(sinhala_vocab),\n",
        "    english_to_index=english_to_index,\n",
        "    sinhala_to_index=sinhala_to_index,\n",
        "    START_TOKEN=START_TOKEN,\n",
        "    END_TOKEN=END_TOKEN,\n",
        "    PADDING_TOKEN=PADDING_TOKEN\n",
        ").to(device)\n",
        "\n",
        "# Print model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total model parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Create train/validation split\n",
        "train_size = int(0.8 * len(english_sentences))\n",
        "val_size = len(english_sentences) - train_size\n",
        "\n",
        "# Shuffle data before splitting\n",
        "combined = list(zip(english_sentences, sinhala_sentences))\n",
        "random.shuffle(combined)\n",
        "english_sentences, sinhala_sentences = zip(*combined)\n",
        "\n",
        "train_english = list(english_sentences[:train_size])\n",
        "train_sinhala = list(sinhala_sentences[:train_size])\n",
        "val_english = list(english_sentences[train_size:])\n",
        "val_sinhala = list(sinhala_sentences[train_size:])\n",
        "\n",
        "print(f\"Train samples: {len(train_english)}\")\n",
        "print(f\"Validation samples: {len(val_english)}\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TranslationDataset(train_english, train_sinhala)\n",
        "val_dataset = TranslationDataset(val_english, val_sinhala)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=sinhala_to_index[PADDING_TOKEN])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JN8uHrXUxrKV"
      },
      "outputs": [],
      "source": [
        "# Helper function to create attention masks\n",
        "def create_masks(src_len, tgt_len, device):\n",
        "    # Create decoder self-attention mask (causal mask)\n",
        "    decoder_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
        "    decoder_mask = decoder_mask.unsqueeze(0).unsqueeze(0).to(device)\n",
        "    return None, decoder_mask, None\n",
        "\n",
        "# Improved training function with better error handling\n",
        "def train_epoch_improved(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        english_batch = batch[\"english\"]\n",
        "        sinhala_batch = batch[\"sinhala\"]\n",
        "\n",
        "        try:\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare input and target sequences\n",
        "            target_input = [START_TOKEN + sentence for sentence in sinhala_batch]\n",
        "            target_output = [sentence + END_TOKEN for sentence in sinhala_batch]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                english_batch,\n",
        "                target_input,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            # Convert target sequences to indices\n",
        "            target_indices = []\n",
        "            for sentence in target_output:\n",
        "                indices = []\n",
        "                for char in sentence:\n",
        "                    if char in sinhala_to_index:\n",
        "                        indices.append(sinhala_to_index[char])\n",
        "                    else:\n",
        "                        indices.append(sinhala_to_index[UNK_TOKEN])\n",
        "\n",
        "                # Pad or truncate to max_sequence_length\n",
        "                if len(indices) > max_sequence_length:\n",
        "                    indices = indices[:max_sequence_length]\n",
        "                else:\n",
        "                    while len(indices) < max_sequence_length:\n",
        "                        indices.append(sinhala_to_index[PADDING_TOKEN])\n",
        "                target_indices.append(indices)\n",
        "\n",
        "            # Convert to tensor\n",
        "            target_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_tensor.reshape(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            print(f\"English batch: {english_batch}\")\n",
        "            print(f\"Sinhala batch: {sinhala_batch}\")\n",
        "            continue\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else 0\n",
        "\n",
        "# Fixed validation function\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for english_batch, sinhala_batch in dataloader:\n",
        "            try:\n",
        "                target_input = [START_TOKEN + sentence for sentence in sinhala_batch]\n",
        "                target_output = [sentence + END_TOKEN for sentence in sinhala_batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    english_batch,\n",
        "                    target_input,\n",
        "                    encoder_self_attention_mask=None,\n",
        "                    decoder_self_attention_mask=None,\n",
        "                    decoder_cross_attention_mask=None,\n",
        "                    enc_start_token=False,\n",
        "                    enc_end_token=False,\n",
        "                    dec_start_token=False,\n",
        "                    dec_end_token=False\n",
        "                )\n",
        "\n",
        "                target_indices = []\n",
        "                for sentence in target_output:\n",
        "                    indices = []\n",
        "                    for char in sentence:\n",
        "                        if char in sinhala_to_index:\n",
        "                            indices.append(sinhala_to_index[char])\n",
        "                        else:\n",
        "                            indices.append(sinhala_to_index[UNK_TOKEN])\n",
        "\n",
        "                    if len(indices) > max_sequence_length:\n",
        "                        indices = indices[:max_sequence_length]\n",
        "                    else:\n",
        "                        while len(indices) < max_sequence_length:\n",
        "                            indices.append(sinhala_to_index[PADDING_TOKEN])\n",
        "                    target_indices.append(indices)\n",
        "\n",
        "                target_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)\n",
        "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_tensor.reshape(-1))\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in validation batch: {e}\")\n",
        "                continue\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets with proper error handling\n",
        "def create_datasets(english_sentences, sinhala_sentences, train_split=0.8):\n",
        "    \"\"\"\n",
        "    Create train and validation datasets with proper error handling\n",
        "    \"\"\"\n",
        "    print(f\"Creating datasets from {len(english_sentences)} sentence pairs...\")\n",
        "\n",
        "    # Validate inputs\n",
        "    if not english_sentences or not sinhala_sentences:\n",
        "        raise ValueError(\"Empty sentence lists provided\")\n",
        "\n",
        "    if len(english_sentences) != len(sinhala_sentences):\n",
        "        raise ValueError(f\"Sentence count mismatch: {len(english_sentences)} vs {len(sinhala_sentences)}\")\n",
        "\n",
        "    # Create indices for train/validation split\n",
        "    num_samples = len(english_sentences)\n",
        "    indices = list(range(num_samples))\n",
        "\n",
        "    # Shuffle indices\n",
        "    import random\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(train_split * num_samples)\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:]\n",
        "\n",
        "    # Create train dataset\n",
        "    train_english = [english_sentences[i] for i in train_indices]\n",
        "    train_sinhala = [sinhala_sentences[i] for i in train_indices]\n",
        "\n",
        "    # Create validation dataset\n",
        "    val_english = [english_sentences[i] for i in val_indices]\n",
        "    val_sinhala = [sinhala_sentences[i] for i in val_indices]\n",
        "\n",
        "    # Create dataset objects\n",
        "    train_dataset = TranslationDataset(train_english, train_sinhala)\n",
        "    val_dataset = TranslationDataset(val_english, val_sinhala)\n",
        "\n",
        "    print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "    print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "Daum6tDoDsQN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6bKsaHhzmHn"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fixed training loop\n",
        "def train_model(model, train_dataset, val_dataset, num_epochs=100, batch_size=32, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete training function with proper error handling\n",
        "    \"\"\"\n",
        "    # Create data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Set to 0 for Colab compatibility\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Set to 0 for Colab compatibility\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    # Initialize optimizer and criterion\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=sinhala_to_index[PADDING_TOKEN])\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training tracking\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    max_patience = 10\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training\n",
        "        train_loss = train_epoch_improved(model, train_dataloader, optimizer, criterion, device)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = validate_epoch(model, val_dataloader, criterion, device)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save model\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'val_loss': val_loss,\n",
        "                'train_loss': train_loss,\n",
        "                'english_to_index': english_to_index,\n",
        "                'sinhala_to_index': sinhala_to_index,\n",
        "                'index_to_english': index_to_english,\n",
        "                'index_to_sinhala': index_to_sinhala,\n",
        "                'english_vocab': english_vocab,\n",
        "                'sinhala_vocab': sinhala_vocab\n",
        "            }, 'best_transformer_model.pth')\n",
        "\n",
        "            print(f\"New best model saved! Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= max_patience:\n",
        "            print(f\"Early stopping triggered after {patience_counter} epochs without improvement!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    # Save vocabularies separately\n",
        "    with open('vocabularies.pkl', 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'english_to_index': english_to_index,\n",
        "            'index_to_english': index_to_english,\n",
        "            'english_vocab': english_vocab,\n",
        "            'sinhala_to_index': sinhala_to_index,\n",
        "            'index_to_sinhala': index_to_sinhala,\n",
        "            'sinhala_vocab': sinhala_vocab\n",
        "        }, f)\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5FIQGXFRDv6R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset, val_dataset = create_datasets(english_sentences, sinhala_sentences)\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=model,  # Your transformer model\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    num_epochs=100,\n",
        "    batch_size=16,  # Reduced for Colab memory constraints\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "print(\"Training completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "nc7aX11xDoHD",
        "outputId": "0e4d488a-100f-4902-a9f9-d677b114470e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets from 342 sentence pairs...\n",
            "Train dataset: 273 samples\n",
            "Validation dataset: 69 samples\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-199877591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_losses, val_losses = train_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Your transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-3710355506.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_dataset, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-847040821.py\u001b[0m in \u001b[0;36mtrain_epoch_improved\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0menglish_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msinhala_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sinhala\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2807\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2808\u001b[0;31m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "O_h970CDxrIB",
        "outputId": "9c8b00f9-2026-4daa-9329-38daf807d789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/35 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-271587840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-19-2488434086.py\u001b[0m in \u001b[0;36mtrain_epoch_improved\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menglish_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinhala_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2808\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-2471378354.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msinhala_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_epoch_improved(model, train_dataloader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation (similar improvements)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for english_batch, sinhala_batch in val_dataloader:\n",
        "            target_input = [START_TOKEN + sentence for sentence in sinhala_batch]\n",
        "            target_output = [sentence + END_TOKEN for sentence in sinhala_batch]\n",
        "\n",
        "            outputs = model(\n",
        "                english_batch,\n",
        "                target_input,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            target_indices = []\n",
        "            for sentence in target_output:\n",
        "                indices = []\n",
        "                for char in sentence:\n",
        "                    if char in sinhala_to_index:\n",
        "                        indices.append(sinhala_to_index[char])\n",
        "                    else:\n",
        "                        indices.append(sinhala_to_index[UNK_TOKEN])\n",
        "\n",
        "                if len(indices) > max_sequence_length:\n",
        "                    indices = indices[:max_sequence_length]\n",
        "                else:\n",
        "                    while len(indices) < max_sequence_length:\n",
        "                        indices.append(sinhala_to_index[PADDING_TOKEN])\n",
        "                target_indices.append(indices)\n",
        "\n",
        "            target_tensor = torch.tensor(target_indices, dtype=torch.long).to(device)\n",
        "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), target_tensor.reshape(-1))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_batches += 1\n",
        "\n",
        "    val_loss = val_loss / val_batches\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'val_loss': val_loss,\n",
        "            'train_loss': train_loss,\n",
        "            'english_to_index': english_to_index,\n",
        "            'sinhala_to_index': sinhala_to_index,\n",
        "            'index_to_english': index_to_english,\n",
        "            'index_to_sinhala': index_to_sinhala,\n",
        "            'english_vocab': english_vocab,\n",
        "            'sinhala_vocab': sinhala_vocab\n",
        "        }, 'best_transformer_model.pth')\n",
        "        print(f\"New best model saved! Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch > 20 and val_loss > best_val_loss * 1.1:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Save vocabularies separately\n",
        "with open('vocabularies.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'english_to_index': english_to_index,\n",
        "        'index_to_english': index_to_english,\n",
        "        'english_vocab': english_vocab,\n",
        "        'sinhala_to_index': sinhala_to_index,\n",
        "        'index_to_sinhala': index_to_sinhala,\n",
        "        'sinhala_vocab': sinhala_vocab\n",
        "    }, f)\n",
        "\n",
        "print(\"Training completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8eTi6J5xysOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb5edf0-79ab-4480-f096-9fb27c7dcd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model...\n",
            "Loaded model from epoch 17 with validation loss 2.5662\n"
          ]
        }
      ],
      "source": [
        "# Fixed translation functions with proper bounds checking\n",
        "def translate_greedy(model, english_sentence, english_to_index, sinhala_to_index, index_to_sinhala, max_length=128):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start with START token\n",
        "        translated = START_TOKEN\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                [english_sentence],\n",
        "                [translated],\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            # Check if we've reached the output sequence limit\n",
        "            if len(translated) >= outputs.shape[1]:\n",
        "                print(f\"Warning: Reached maximum output sequence length ({outputs.shape[1]})\")\n",
        "                break\n",
        "\n",
        "            # Get the next character prediction\n",
        "            next_char_logits = outputs[0, len(translated), :]\n",
        "            next_char_id = torch.argmax(next_char_logits).item()\n",
        "            next_char = index_to_sinhala[next_char_id]\n",
        "\n",
        "            # Stop if END token is generated\n",
        "            if next_char == END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # Add to translated sentence\n",
        "            translated += next_char\n",
        "\n",
        "        # Remove START token\n",
        "        translated = translated.replace(START_TOKEN, '')\n",
        "        return translated\n",
        "\n",
        "def translate_beam_search(model, english_sentence, english_to_index, sinhala_to_index, index_to_sinhala, beam_size=3, max_length=128):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Initialize beam with START token\n",
        "        beams = [(START_TOKEN, 0.0)]  # (sequence, score)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            new_beams = []\n",
        "\n",
        "            for sequence, score in beams:\n",
        "                # Check if sequence ended\n",
        "                if sequence.endswith(END_TOKEN):\n",
        "                    new_beams.append((sequence, score))\n",
        "                    continue\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    [english_sentence],\n",
        "                    [sequence],\n",
        "                    encoder_self_attention_mask=None,\n",
        "                    decoder_self_attention_mask=None,\n",
        "                    decoder_cross_attention_mask=None,\n",
        "                    enc_start_token=False,\n",
        "                    enc_end_token=False,\n",
        "                    dec_start_token=False,\n",
        "                    dec_end_token=False\n",
        "                )\n",
        "\n",
        "                # Check if we've reached the output sequence limit\n",
        "                if len(sequence) >= outputs.shape[1]:\n",
        "                    new_beams.append((sequence + END_TOKEN, score))\n",
        "                    continue\n",
        "\n",
        "                # Get probabilities for next character\n",
        "                next_char_logits = outputs[0, len(sequence), :]\n",
        "                next_char_probs = torch.softmax(next_char_logits, dim=-1)\n",
        "\n",
        "                # Get top k candidates\n",
        "                top_k_probs, top_k_indices = torch.topk(next_char_probs, beam_size)\n",
        "\n",
        "                # Create new beam candidates\n",
        "                for prob, idx in zip(top_k_probs, top_k_indices):\n",
        "                    next_char = index_to_sinhala[idx.item()]\n",
        "                    new_sequence = sequence + next_char\n",
        "                    new_score = score + torch.log(prob).item()\n",
        "                    new_beams.append((new_sequence, new_score))\n",
        "\n",
        "            # Select top beam_size beams\n",
        "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "            # Check if all beams ended\n",
        "            if all(seq.endswith(END_TOKEN) for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "        # Return best sequence\n",
        "        best_sequence = beams[0][0]\n",
        "        # Remove special tokens\n",
        "        best_sequence = best_sequence.replace(START_TOKEN, '').replace(END_TOKEN, '')\n",
        "        return best_sequence\n",
        "\n",
        "# Alternative approach: Use a smaller max_length that matches your model's training\n",
        "def translate_greedy_safe(model, english_sentence, english_to_index, sinhala_to_index, index_to_sinhala, max_length=60):\n",
        "    \"\"\"\n",
        "    Safer version that uses a max_length smaller than the model's sequence limit\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start with START token\n",
        "        translated = START_TOKEN\n",
        "\n",
        "        for step in range(max_length):\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                [english_sentence],\n",
        "                [translated],\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False,\n",
        "                dec_end_token=False\n",
        "            )\n",
        "\n",
        "            # Get the next character prediction\n",
        "            current_pos = len(translated)\n",
        "            if current_pos >= outputs.shape[1]:\n",
        "                print(f\"Warning: Reached model's sequence limit at step {step}\")\n",
        "                break\n",
        "\n",
        "            next_char_logits = outputs[0, current_pos, :]\n",
        "            next_char_id = torch.argmax(next_char_logits).item()\n",
        "            next_char = index_to_sinhala[next_char_id]\n",
        "\n",
        "            # Stop if END token is generated\n",
        "            if next_char == END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # Add to translated sentence\n",
        "            translated += next_char\n",
        "\n",
        "        # Remove START token\n",
        "        translated = translated.replace(START_TOKEN, '')\n",
        "        return translated\n",
        "\n",
        "# Load the best model\n",
        "print(\"Loading best model...\")\n",
        "checkpoint = torch.load('best_transformer_model.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded model from epoch {checkpoint['epoch']} with validation loss {checkpoint['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNYIL3IVzpev"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cEZjnJxoysL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f233b1e2-680f-458b-ff4f-3ce91482ff4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TESTING TRANSLATION\n",
            "==================================================\n",
            "Warning: Reached maximum output sequence length (64)\n",
            "English: How much does it cost?\n",
            "Expected: මේක කීයද?\n",
            "Generated: ><UNK><UNK><UNK><UNK><UNK>>><UNK>><UNK><UNK>>><UNK>><UNK>\n",
            "--------------------------------------------------\n",
            "Warning: Reached maximum output sequence length (64)\n",
            "English: I am sad\n",
            "Expected: මට දුකයි\n",
            "Generated: <UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
            "--------------------------------------------------\n",
            "Warning: Reached maximum output sequence length (64)\n",
            "English: Thank you\n",
            "Expected: ස්තූතියි\n",
            "Generated: <UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
            "--------------------------------------------------\n",
            "Warning: Reached maximum output sequence length (64)\n",
            "English: They are students\n",
            "Expected: ඔවුන් සිසුන්\n",
            "Generated: <UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>ිි\n",
            "--------------------------------------------------\n",
            "Warning: Reached maximum output sequence length (64)\n",
            "English: I like this food\n",
            "Expected: මට මේ කෑම කමතියි\n",
            "Generated: ><UNK><UNK><UNK><UNK><UNK>><UNK>><UNK><UNK>>>><UNK><UNK>ය\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test translation function\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TESTING TRANSLATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test with training samples\n",
        "test_indices = random.sample(range(len(val_english)), min(5, len(val_english)))\n",
        "\n",
        "for idx in test_indices:\n",
        "    english_text = val_english[idx]\n",
        "    expected_sinhala = val_sinhala[idx]\n",
        "\n",
        "    translation_greedy = translate_greedy(model, english_text, english_to_index, sinhala_to_index, index_to_sinhala)\n",
        "\n",
        "    print(f\"English: {english_text}\")\n",
        "    print(f\"Expected: {expected_sinhala}\")\n",
        "    print(f\"Generated: {translation_greedy}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "awXJ0iqW0_iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11490f0-ca64-4731-d879-f16588677387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "INTERACTIVE TRANSLATION\n",
            "==================================================\n",
            "Enter English text to translate (type 'quit' to exit):\n",
            "\n",
            "English: Hi\n",
            "Warning: Reached maximum output sequence length (64)\n",
            "Sinhala (Greedy): <UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
            "\n",
            "English: quit\n",
            "Translation session ended!\n"
          ]
        }
      ],
      "source": [
        "# Interactive translation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INTERACTIVE TRANSLATION\")\n",
        "print(\"=\"*50)\n",
        "print(\"Enter English text to translate (type 'quit' to exit):\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nEnglish: \").strip()\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Try both translation methods\n",
        "        translation_greedy = translate_greedy(model, user_input, english_to_index, sinhala_to_index, index_to_sinhala)\n",
        "        print(f\"Sinhala (Greedy): {translation_greedy}\")\n",
        "\n",
        "        # Uncomment the line below for beam search (slower but potentially better)\n",
        "        # translation_beam = translate_beam_search(model, user_input, english_to_index, sinhala_to_index, index_to_sinhala)\n",
        "        # print(f\"Sinhala (Beam): {translation_beam}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"This might happen if the input contains characters not in the vocabulary.\")\n",
        "\n",
        "print(\"Translation session ended!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOFTMx7W1Bc8"
      },
      "outputs": [],
      "source": [
        "# Save final model info\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total epochs: {num_epochs}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"English vocabulary size: {len(english_vocab)}\")\n",
        "print(f\"Sinhala vocabulary size: {len(sinhala_vocab)}\")\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}